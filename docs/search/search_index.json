{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Pocket TTS","text":"<p>A lightweight text-to-speech (TTS) application designed to run efficiently on CPUs. Forget about the hassle of using GPUs and web APIs serving TTS models. With Kyutai's Pocket TTS, generating audio is just a pip install and a function call away.</p> <p>Supports Python 3.10, 3.11, 3.12, 3.13 and 3.14. Requires PyTorch 2.5+. Does not require the gpu version of PyTorch.</p> <p>\ud83d\udd0a Demo | \ud83d\udc31\u200d\ud83d\udcbbGitHub Repository | \ud83e\udd17 Hugging Face Model Card | \u2699\ufe0f Tech report | \ud83d\udcc4 Paper | \ud83d\udcda Documentation</p>"},{"location":"#main-takeaways","title":"Main takeaways","text":"<ul> <li>Runs on CPU</li> <li>Small model size, 100M parameters</li> <li>Audio streaming</li> <li>Low latency, ~200ms to get the first audio chunk</li> <li>Faster than real-time, ~6x real-time on a CPU of MacBook Air M4</li> <li>Uses only 2 CPU cores</li> <li>Python API and CLI</li> <li>Voice cloning</li> <li>English only at the moment</li> <li>Can handle infinitely long text inputs</li> <li>Can run on client-side in the browser</li> </ul>"},{"location":"#trying-it-from-the-website-without-installing-anything","title":"Trying it from the website, without installing anything","text":"<p>Navigate to the Kyutai website to try it out directly in your browser. You can input text, select different voices, and generate speech without any installation.</p>"},{"location":"#trying-it-with-the-cli","title":"Trying it with the CLI","text":""},{"location":"#the-generate-command","title":"The <code>generate</code> command","text":"<p>You can use pocket-tts directly from the command line. We recommend using <code>uv</code> as it installs any dependencies on the fly in an isolated environment (uv installation instructions here). You can also use <code>pip install pocket-tts</code> to install it manually.</p> <p>This will generate a wav file <code>./tts_output.wav</code> saying the default text with the default voice, and display some speed statistics.</p> <pre><code>uvx pocket-tts generate\n# or if you installed it manually with pip:\npocket-tts generate\n</code></pre> <p>Modify the voice with <code>--voice</code> and the text with <code>--text</code>. We provide a small catalog of voices.</p> <p>You can take a look at this page which details the licenses for each voice.</p> <ul> <li>alba</li> <li>marius</li> <li>javert</li> <li>jean</li> <li>fantine</li> <li>cosette</li> <li>eponine</li> <li>azelma</li> </ul> <p>The <code>--voice</code> argument can also take a plain wav file as input for voice cloning. You can use your own or check out our voice repository. We recommend cleaning the sample before using it with Pocket TTS, because the audio quality of the sample is also reproduced.</p> <p>Feel free to check out the generate documentation for more details and examples. For trying multiple voices and prompts quickly, prefer using the <code>serve</code> command.</p>"},{"location":"#the-serve-command","title":"The <code>serve</code> command","text":"<p>You can also run a local server to generate audio via HTTP requests.</p> <pre><code>uvx pocket-tts serve\n# or if you installed it manually with pip:\npocket-tts serve\n</code></pre> <p>Navigate to <code>http://localhost:8000</code> to try the web interface, it's faster than the command line as the model is kept in memory between requests.</p> <p>You can check out the serve documentation for more details and examples.</p>"},{"location":"#the-export-voice-command","title":"The <code>export-voice</code> command","text":"<p>Processing an audio file (e.g., a .wav or .mp3) for voice cloning is relatively slow, but loading a safetensors file -- a voice embedding converted from an audio file -- is very fast. You can use the <code>export-voice</code> command to do this conversion. See the export-voice documentation for more details and examples.</p>"},{"location":"#using-it-as-a-python-library","title":"Using it as a Python library","text":"<p>You can try out the Python library on Colab here.</p> <p>Install the package with</p> <pre><code>pip install pocket-tts\n# or\nuv add pocket-tts\n</code></pre> <p>You can use this package as a simple Python library to generate audio from text.</p> <pre><code>from pocket_tts import TTSModel\nimport scipy.io.wavfile\n\ntts_model = TTSModel.load_model()\nvoice_state = tts_model.get_state_for_audio_prompt(\n    \"alba\"  # One of the pre-made voices, see above\n    # You can also use any voice file you have locally or from Hugging Face:\n    # \"./some_audio.wav\"\n    # or \"hf://kyutai/tts-voices/expresso/ex01-ex02_default_001_channel2_198s.wav\"\n)\naudio = tts_model.generate_audio(voice_state, \"Hello world, this is a test.\")\n# Audio is a 1D torch tensor containing PCM data.\nscipy.io.wavfile.write(\"output.wav\", tts_model.sample_rate, audio.numpy())\n</code></pre> <p>You can have multiple voice states around if you have multiple voices you want to use. <code>load_model()</code> and <code>get_state_for_audio_prompt()</code> are relatively slow operations, so we recommend to keep the model and voice states in memory if you can.</p> <p>You can check out the Python API documentation for more details and examples.</p>"},{"location":"#unsupported-features","title":"Unsupported features","text":"<p>At the moment, we do not support (but would love pull requests adding):</p> <ul> <li>Running the TTS inside a web browser (WebAssembly)</li> <li>A compiled version with for example <code>torch.compile()</code> or <code>candle</code>.</li> <li>Adding silence in the text input to generate pauses.</li> <li>Quantization to run the computation in int8.</li> </ul> <p>We tried running this TTS model on the GPU but did not observe a speedup compared to CPU execution, notably because we use a batch size of 1 and a very small model.</p>"},{"location":"#development-and-local-setup","title":"Development and local setup","text":"<p>We accept contributions! Feel free to open issues or pull requests on GitHub.</p> <p>You can find development instructions in the CONTRIBUTING.md file. You'll also find there how to have an editable install of the package for local development.</p>"},{"location":"#in-browser-implementations","title":"In-browser implementations","text":"<p>Pocket TTS is small enough to run directly in your browser in WebAssembly/JavaScript. We don't have official support for this yet, but you can try out one of these community implementations:</p> <ul> <li>babybirdprd/pocket-tts: Candle version (Rust) with WebAssembly and PyO3 bindings, meaning it can run on the web too.</li> <li>ekzhang/jax-js: Using jax-js, a ML library for the web. Demo here</li> <li>KevinAHM/pocket-tts-onnx-export: Model exported to .onnx and run using ONNX Runtime Web. Demo here</li> </ul>"},{"location":"#alterative-implementations","title":"Alterative implementations","text":"<ul> <li>jishnuvenugopal/pocket-tts-mlx - MLX backend optimized for Apple Silicon</li> <li>babybirdprd/pocket-tts - Candle version (Rust) with WebAssembly and PyO3 bindings.</li> </ul>"},{"location":"#projects-using-pocket-tts","title":"Projects using Pocket TTS","text":"<ul> <li>lukasmwerner/pocket-reader - Browser screen reader</li> <li>ikidd/pocket-tts-wyoming - Docker container for pocket-tts using Wyoming protocol, ready for Home Assistant Voice use.</li> <li>slaughters85j/pocket-tts - Mac Desktop App + macOS Quick Action</li> <li>teddybear082/pocket-tts-openai_streaming_server - OpenAI-compatible streaming server, dockerized and with an <code>.exe</code> release</li> </ul>"},{"location":"#prohibited-use","title":"Prohibited use","text":"<p>Use of our model must comply with all applicable laws and regulations and must not result in, involve, or facilitate any illegal, harmful, deceptive, fraudulent, or unauthorized activity. Prohibited uses include, without limitation, voice impersonation or cloning without explicit and lawful consent; misinformation, disinformation, or deception (including fake news, fraudulent calls, or presenting generated content as genuine recordings of real people or events); and the generation of unlawful, harmful, libelous, abusive, harassing, discriminatory, hateful, or privacy-invasive content. We disclaim all liability for any non-compliant use.</p>"},{"location":"#authors","title":"Authors","text":"<p>Manu Orsini, Simon Rouard, Gabriel De Marmiesse*, V\u00e1clav Volhejn, Neil Zeghidour, Alexandre D\u00e9fossez</p> <p>*equal contribution</p>"},{"location":"API%20Reference/python-api/","title":"Python API Documentation","text":"<p>Kyutai Pocket TTS provides a Python API for integrating text-to-speech capabilities into your applications.</p>"},{"location":"API%20Reference/python-api/#installation","title":"Installation","text":"<pre><code>pip install pocket-tts\n</code></pre>"},{"location":"API%20Reference/python-api/#quick-start","title":"Quick Start","text":"<pre><code>from pocket_tts import TTSModel\nimport scipy.io.wavfile\n\n# Load the model\ntts_model = TTSModel.load_model()\n\n# Get voice state from an audio file\nvoice_state = tts_model.get_state_for_audio_prompt(\n    \"hf://kyutai/tts-voices/alba-mackenna/casual.wav\"\n)\n\n# Generate audio\naudio = tts_model.generate_audio(voice_state, \"Hello world, this is a test.\")\n\n# Save to file\nscipy.io.wavfile.write(\"output.wav\", tts_model.sample_rate, audio.numpy())\n</code></pre>"},{"location":"API%20Reference/python-api/#core-classes","title":"Core Classes","text":""},{"location":"API%20Reference/python-api/#ttsmodel","title":"TTSModel","text":"<p>The main class for text-to-speech generation.</p>"},{"location":"API%20Reference/python-api/#class-methods","title":"Class Methods","text":""},{"location":"API%20Reference/python-api/#load_modelconfigb6369a24-temp07-lsd_decode_steps1-noise_clampnone-eos_threshold-40","title":"<code>load_model(config=\"b6369a24\", temp=0.7, lsd_decode_steps=1, noise_clamp=None, eos_threshold=-4.0)</code>","text":"<p>Load and return a TTSModel instance with pre-trained weights.</p> <p>Parameters: - <code>config</code> (str): Path to model config YAML file or a variant identifier (default: \"b6369a24\") - <code>temp</code> (float): Sampling temperature for generation (default: 0.7) - <code>lsd_decode_steps</code> (int): Number of generation steps (default: 1) - <code>noise_clamp</code> (float | None): Maximum value for noise sampling (default: None) - <code>eos_threshold</code> (float): Threshold for end-of-sequence detection (default: -4.0)</p> <p>Returns: - <code>TTSModel</code>: Loaded model instance on CPU</p> <p>Example: <pre><code>from pocket_tts import TTSModel\n\n# Load with default settings\nmodel = TTSModel.load_model()\n\n# Load with custom parameters\nmodel = TTSModel.load_model(variant=\"b6369a24\", temp=0.5, lsd_decode_steps=5, eos_threshold=-3.0)\n</code></pre></p>"},{"location":"API%20Reference/python-api/#properties","title":"Properties","text":""},{"location":"API%20Reference/python-api/#device-str","title":"<code>device</code> (str)","text":"<p>Returns the device type where the model is running (\"cpu\" or \"cuda\"). By default, the model runs on CPU.</p> <pre><code>from pocket_tts import TTSModel\n\nmodel = TTSModel.load_model()\nprint(f\"Model running on: {model.device}\")\n</code></pre>"},{"location":"API%20Reference/python-api/#sample_rate-int","title":"<code>sample_rate</code> (int)","text":"<p>Returns the generated audio sample rate (typically 24000 Hz).</p> <pre><code>from pocket_tts import TTSModel\n\nmodel = TTSModel.load_model()\nprint(f\"Sample rate: {model.sample_rate} Hz\")\n</code></pre>"},{"location":"API%20Reference/python-api/#methods","title":"Methods","text":""},{"location":"API%20Reference/python-api/#get_state_for_audio_promptaudio_conditioning-truncatefalse","title":"<code>get_state_for_audio_prompt(audio_conditioning, truncate=False)</code>","text":"<p>Extract model state for a given audio file or URL (voice cloning), or load from a .safetensors file.</p> <p>Parameters: - <code>audio_conditioning</code> (Path | str | torch.Tensor): Audio or .safetensors file path, URL, or tensor - <code>truncate</code> (bool): Whether to truncate the audio (default: False)</p> <p>Returns: - <code>dict</code>: Model state dictionary containing hidden states and positional information</p> <p>Example: <pre><code>from pocket_tts import TTSModel\n\nmodel = TTSModel.load_model()\n# From HuggingFace URL\nvoice_state = model.get_state_for_audio_prompt(\"hf://kyutai/tts-voices/alba-mackenna/casual.wav\")\n\n# From local file\nvoice_state = model.get_state_for_audio_prompt(\"./my_voice.wav\")\n\n# Reload state from a .safetensors file (much faster than extracting from an audio file)\nvoice_state = model.get_state_for_audio_prompt(\"./my_voices.safetensors\")\n\n# From HTTP URL\nvoice_state = model.get_state_for_audio_prompt(\n    \"https://huggingface.co/kyutai/tts-voices/resolve\"\n    \"/main/expresso/ex01-ex02_default_001_channel1_168s.wav\"\n)\n</code></pre></p>"},{"location":"API%20Reference/python-api/#generate_audiomodel_state-text_to_generate-frames_after_eosnone-copy_statetrue","title":"<code>generate_audio(model_state, text_to_generate, frames_after_eos=None, copy_state=True)</code>","text":"<p>Generate complete audio tensor from text input.</p> <p>Parameters: - <code>model_state</code> (dict): Model state from <code>get_state_for_audio_prompt()</code> - <code>text_to_generate</code> (str): Text to convert to speech - <code>frames_after_eos</code> (int | None): Frames to generate after EOS detection (default: None) - <code>copy_state</code> (bool): Whether to copy the state (default: True)</p> <p>Returns: - <code>torch.Tensor</code>: Audio 1D tensor with shape [samples]</p> <p>Example: <pre><code>from pocket_tts import TTSModel\n\nmodel = TTSModel.load_model()\n\nvoice_state = model.get_state_for_audio_prompt(\"hf://kyutai/tts-voices/alba-mackenna/casual.wav\")\n\n# Generate audio\naudio = model.generate_audio(voice_state, \"Hello world!\", frames_after_eos=2, copy_state=True)\n\nprint(f\"Generated audio shape: {audio.shape}\")\nprint(f\"Audio duration: {audio.shape[-1] / model.sample_rate:.2f} seconds\")\n</code></pre></p>"},{"location":"API%20Reference/python-api/#generate_audio_streammodel_state-text_to_generate-frames_after_eosnone-copy_statetrue","title":"<code>generate_audio_stream(model_state, text_to_generate, frames_after_eos=None, copy_state=True)</code>","text":"<p>Generate audio streaming chunks from text input.</p> <p>Parameters: Same as <code>generate_audio()</code></p> <p>Yields: - <code>torch.Tensor</code>: Audio chunks with shape [samples]</p> <p>Example: <pre><code>from pocket_tts import TTSModel\n\nmodel = TTSModel.load_model()\n\nvoice_state = model.get_state_for_audio_prompt(\"hf://kyutai/tts-voices/alba-mackenna/casual.wav\")\n# Stream generation\nfor chunk in model.generate_audio_stream(voice_state, \"Long text content...\"):\n    # Process each chunk as it's generated\n    print(f\"Generated chunk: {chunk.shape[0]} samples\")\n    # Could save chunks to file or play in real-time\n</code></pre></p>"},{"location":"API%20Reference/python-api/#save_audio_promptaudio_conditioning-export_path-truncatefalse","title":"<code>save_audio_prompt(audio_conditioning, export_path, truncate=False)</code>","text":"<p>Save audio prompt to a .safetensors file.</p> <p>Parameters: - <code>audio_conditioning</code> (Path | str | torch.Tensor): Audio file path, URL, or tensor - <code>export_path</code> (Path | str): .safetensors file path - <code>truncate</code> (bool): Whether to truncate the audio (default: False)</p> <p>Returns: - tensor of the converted audio.</p> <p>Example: <pre><code>from pocket_tts import TTSModel\n\nmodel = TTSModel.load_model()\n# From HuggingFace URL\nmodel.get_state_for_audio_prompt(\n    \"hf://kyutai/tts-voices/alba-mackenna/casual.wav\", \"casual.safetensors\"\n)\n\n# From local file (the .safetensors extension will be added automatically)\ntensor = model.get_state_for_audio_prompt(\"./my_voice.wav\", \"my_voice\")\n\n# Use the tensor, Luke!\naudio = model.generate_audio(tensor, \"Hello world!\")\n</code></pre></p>"},{"location":"API%20Reference/python-api/#advanced-usage","title":"Advanced Usage","text":""},{"location":"API%20Reference/python-api/#voice-management","title":"Voice Management","text":"<pre><code>from pocket_tts import TTSModel\n\nmodel = TTSModel.load_model()\n# Preload multiple voices\nvoices = {\n    \"casual\": model.get_state_for_audio_prompt(\"hf://kyutai/tts-voices/alba-mackenna/casual.wav\"),\n    \"funny\": model.get_state_for_audio_prompt(\n        \"https://huggingface.co/kyutai/tts-voices/resolve/main/expresso/ex01-ex02_default_001_channel1_168s.wav\"\n    ),\n}\n\n# Generate with different voices\ncasual_audio = model.generate_audio(voices[\"casual\"], \"Hey there!\")\nfunny_audio = model.generate_audio(voices[\"funny\"], \"Good morning.\")\n</code></pre>"},{"location":"API%20Reference/python-api/#batch-processing","title":"Batch Processing","text":"<pre><code>from pocket_tts import TTSModel\nimport scipy.io.wavfile\nimport torch\n\nmodel = TTSModel.load_model()\n\nvoice_state = model.get_state_for_audio_prompt(\"hf://kyutai/tts-voices/alba-mackenna/casual.wav\")\n# Process multiple texts efficiently by re-using the same voice state\ntexts = [\n    \"First sentence to generate.\",\n    \"Second sentence to generate.\",\n    \"Third sentence to generate.\",\n]\n\naudios = []\nfor text in texts:\n    audio = model.generate_audio(voice_state, text)\n    audios.append(audio)\n\n# Concatenate all audio\nfull_audio = torch.cat(audios, dim=0)\nscipy.io.wavfile.write(\"batch_output.wav\", model.sample_rate, full_audio.numpy())\n</code></pre>"},{"location":"API%20Reference/python-api/#streaming-to-file","title":"Streaming to File","text":"<p>You can refer to our CLI implementation which can stream audio to a wav file.</p> <p>For more information about the command-line interface, see the Generate Documentation or Serve Documentation.</p>"},{"location":"CLI%20Commands/export_voice/","title":"Export Voice","text":"<p>Kyutai Pocket TTS allows you to generate speech from an audio sample. However, processing an audio file each time is relatively slow and inefficient.</p> <p>The <code>export-voice</code> command allows you to convert an audio file to a voice embedding in safetensors format. The safetensors file can then be loaded very quickly whenever you generate speech.</p>"},{"location":"CLI%20Commands/export_voice/#basic-usage","title":"Basic Usage","text":"<pre><code>uvx pocket-tts export-voice audio-path export-path\n# or if installed manually:\npocket-tts export-voice audio-path export-path\n</code></pre>"},{"location":"CLI%20Commands/export_voice/#command-options","title":"Command Options","text":""},{"location":"CLI%20Commands/export_voice/#required-parameters","title":"Required Parameters","text":"<ul> <li><code>audio-path</code>: Path of the audio file or directory to convert. <code>audio-path</code> can point to an <code>http:</code> or <code>hf:</code> (hugging face) file. If <code>audio-path</code> is a local directory, all audio files found inside it will be batch converted. Supports popular audio file formats like wav and mp3.</li> <li><code>export-path</code>: Path of safetensors file or directory to export to. For batch conversion, export-path should be a directory. The directory will be created if it does not exist.</li> </ul>"},{"location":"CLI%20Commands/export_voice/#options","title":"Options","text":"<ul> <li><code>--truncate</code>: Automatically truncate long audio files down to 30 seconds.</li> </ul> <p>The other parameters such as <code>--lsd-decode-steps</code> and <code>--temperature</code> are the same as for the <code>generate</code> command. See the generate documentation for more details.</p>"},{"location":"CLI%20Commands/export_voice/#examples","title":"Examples","text":"<pre><code># export a single file\npocket-tts export-voice voice_memo127762.mp3 jack.safetensors\n\n# export a single file to a different directory (output is embbeddings/mary.safetensors\npocket-tts export-voice voices/mary.wav embeddings/\n\n# export an entire directory of audio files, truncate long audios\npocket-tts export-voice voices/ embeddings/ --truncate\n\n# export an online file to current directory\npocket-tts export-voice https://huggingface.co/kyutai/tts-voices/resolve/main/alba-mackenna/announcer.wav .\n\n# use the exported safetensors\npocket-tts generate --text \"Hello, welcome to today's game between the Bears and Cubs.\"  --voice announcer.safetensors\n</code></pre> <p>Note: to indicate a directory rather than a file, please be sure to include a trailing / (\\ on Windows).</p>"},{"location":"CLI%20Commands/generate/","title":"Generate","text":"<p>The <code>generate</code> command allows you to generate speech from text directly from the command line using Kyutai Pocket TTS.</p>"},{"location":"CLI%20Commands/generate/#basic-usage","title":"Basic Usage","text":"<pre><code>uvx pocket-tts generate\n# or if installed manually:\npocket-tts generate\n</code></pre> <p>This will generate a WAV file <code>./tts_output.wav</code> with the default text and voice.</p>"},{"location":"CLI%20Commands/generate/#command-options","title":"Command Options","text":""},{"location":"CLI%20Commands/generate/#core-options","title":"Core Options","text":"<ul> <li><code>--text TEXT</code>: Text to generate (default: \"Hello world! I am Kyutai Pocket TTS. I'm fast enough to run on small CPUs. I hope you'll like me.\")</li> <li><code>--voice VOICE</code>: Path to audio conditioning file (voice to clone) (default: \"hf://kyutai/tts-voices/alba-mackenna/casual.wav\"). Urls and local paths are supported.</li> <li><code>--output-path OUTPUT_PATH</code>: Output path for generated audio (default: \"./tts_output.wav\")</li> </ul>"},{"location":"CLI%20Commands/generate/#generation-parameters","title":"Generation Parameters","text":"<ul> <li><code>--config CONFIG_PATH</code>: Path to custom config.yaml (for loading local model files) or model signature (default: \"b6369a24\")</li> <li><code>--lsd-decode-steps LSD_DECODE_STEPS</code>: Number of generation steps (default: 1)</li> <li><code>--temperature TEMPERATURE</code>: Temperature for generation (default: 0.7)</li> <li><code>--noise-clamp NOISE_CLAMP</code>: Noise clamp value (default: None)</li> <li><code>--eos-threshold EOS_THRESHOLD</code>: EOS threshold (default: -4.0)</li> <li><code>--frames-after-eos FRAMES_AFTER_EOS</code>: Number of frames to generate after EOS (default: None, auto-calculated based on the text length). Each frame is 80ms.</li> </ul>"},{"location":"CLI%20Commands/generate/#performance-options","title":"Performance Options","text":"<ul> <li><code>--device DEVICE</code>: Device to use (default: \"cpu\", you may not get a speedup by using a gpu since it's a small model)</li> <li><code>--quiet</code>, <code>-q</code>: Disable logging output</li> </ul>"},{"location":"CLI%20Commands/generate/#examples","title":"Examples","text":""},{"location":"CLI%20Commands/generate/#basic-generation","title":"Basic Generation","text":"<pre><code># Generate with default settings\npocket-tts generate\n\n# Custom text\npocket-tts generate --text \"Hello, this is a custom message.\"\n\n# Custom output path\npocket-tts generate --output-path \"./my_audio.wav\"\n</code></pre>"},{"location":"CLI%20Commands/generate/#voice-selection","title":"Voice Selection","text":"<pre><code># Use different voice from HuggingFace\npocket-tts generate --voice \"hf://kyutai/tts-voices/jessica-jian/casual.wav\"\n\n# Use local voice file\npocket-tts generate --voice \"./my_voice.wav\"\n\n# Use a safetensors file (such as one created using `pocket-tts export-voice`)\npocket-tts generate --voice \"./my_voice.safetensors\"\n</code></pre>"},{"location":"CLI%20Commands/generate/#quality-tuning","title":"Quality Tuning","text":"<pre><code># Higher quality (more steps)\npocket-tts generate --lsd-decode-steps 5 --temperature 0.5\n\n# More expressive (higher temperature)\npocket-tts generate --temperature 1.0\n\n# Adjust EOS threshold, smaller means finishing earlier.\npocket-tts generate --eos-threshold -3.0\n</code></pre>"},{"location":"CLI%20Commands/generate/#custom-model-config","title":"Custom Model Config","text":"<p>If you'd like to override the paths from which the models are loaded, you can provide a custom YAML configuration.</p> <p>Copy pocket_tts/config/b6369a24.yaml and change weights_path:, weights_path_without_voice_cloning: and tokenizer_path: to the paths of the models you want to load.</p> <p>Then, use the --config option to point to your newly created config.</p> <pre><code># Use a different config\npocket-tts generate --config \"C://pocket-tts/my_config.yaml\"\n</code></pre>"},{"location":"CLI%20Commands/generate/#output-format","title":"Output Format","text":"<p>The generate command always outputs WAV files in the following format:</p> <ul> <li>Sample Rate: 24kHz</li> <li>Channels: Mono</li> <li>Bit Depth: 16-bit PCM</li> <li>Format: Standard WAV file</li> </ul> <p>For more advanced usage, see the Python API documentation or consider using the serve command for web-based generation and quick iteration.</p>"},{"location":"CLI%20Commands/serve/","title":"Serve","text":"<p>The <code>serve</code> command starts a FastAPI web server that provides both a web interface and HTTP API for text-to-speech generation.</p>"},{"location":"CLI%20Commands/serve/#basic-usage","title":"Basic Usage","text":"<pre><code>uvx pocket-tts serve\n# or if installed manually:\npocket-tts serve\n</code></pre> <p>This starts a server on <code>http://localhost:8000</code> with the default voice model.</p>"},{"location":"CLI%20Commands/serve/#command-options","title":"Command Options","text":"<ul> <li><code>--voice VOICE</code>: Path to voice prompt audio file (voice to clone) (default: \"hf://kyutai/tts-voices/alba-mackenna/casual.wav\")</li> <li><code>--host HOST</code>: Host to bind to (default: \"localhost\")</li> <li><code>--port PORT</code>: Port to bind to (default: 8000)</li> <li><code>--reload</code>: Enable auto-reload for development</li> <li><code>--config</code>: Path to a custom config .yaml</li> </ul>"},{"location":"CLI%20Commands/serve/#examples","title":"Examples","text":""},{"location":"CLI%20Commands/serve/#basic-server","title":"Basic Server","text":"<pre><code># Start with default settings\npocket-tts serve\n\n# Custom host and port\npocket-tts serve --host \"localhost\" --port 8080\n</code></pre>"},{"location":"CLI%20Commands/serve/#custom-voice","title":"Custom Voice","text":"<pre><code># Use different voice\npocket-tts serve --voice \"hf://kyutai/tts-voices/jessica-jian/casual.wav\"\n\n# Use local voice file\npocket-tts serve --voice \"./my_voice.wav\"\n</code></pre>"},{"location":"CLI%20Commands/serve/#custom-model-config","title":"Custom Model Config","text":"<p>If you'd like to override the paths from which the models are loaded, you can provide a custom YAML configuration.</p> <p>Copy pocket_tts/config/b6369a24.yaml and change weights_path:, weights_path_without_voice_cloning: and tokenizer_path: to the paths of the models you want to load.</p> <p>Then, use the --config option to point to your newly created config.</p> <pre><code># Use a different config\npocket-tts serve --config \"C://pocket-tts/my_config.yaml\"\n</code></pre>"},{"location":"CLI%20Commands/serve/#web-interface","title":"Web Interface","text":"<p>Once the server is running, navigate to <code>http://localhost:8000</code> to access the web interface.</p> <p>For more advanced usage, see the Python API documentation for direct integration with the TTS model.</p>"}]}